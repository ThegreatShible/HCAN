{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpEggSKlUCjk"
   },
   "source": [
    "# DEMO\n",
    "This demo does a supervised learning classification on the corpus in `corpus.xlsx` using a simple CNN model then HCAN with 1 and 2 hierarchies. Of course a data pretreatment and cleaning comes first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByeHSnyXUJEz"
   },
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "blm4V8p0nwqI",
    "outputId": "a983e97a-d602-4f11-fab3-6986593667c0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import  tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, concatenate, Dense, Activation, Dropout, Softmax, Layer\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os\n",
    "import sys\n",
    "module_path = '../src'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from HCAN import *\n",
    "from simple_CNN import *\n",
    "import setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = \"unknowntoken\"\n",
    "CURRENCY_TOKEN = \"currencytoken\"\n",
    "EMBEDDING_DIM=50\n",
    "embedding_root_dir = setup.embedding_root_dir\n",
    "words_embedding_path = os.path.join(embedding_root_dir, \"glove.6B.{}d.txt\".format(EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkAZgLk4wGCX"
   },
   "source": [
    "# Text representation and pretreatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xQGR_jGcytww"
   },
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhNDEwCazWl8"
   },
   "outputs": [],
   "source": [
    "corpus_path = 'corpus.xlsx'\n",
    "corpus =pd.read_excel(corpus_path, index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-3x1fFGyxVu"
   },
   "source": [
    "## Cleaning data\n",
    "Clean the data by : \n",
    "* Remove special caracters\n",
    "* Replace words of type (Anyword's, he'll, should've....) by splitting on the quotation mark => (Anyword 's,he 'll, should 've) so that it becomes two words\n",
    "* Replace currencies by ```CURRENCY_TOKEN```\n",
    "And Then split by special caracters for each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yU-6RmSF1EIG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def pretreat(line) :\n",
    "    line = re.sub(r'[…―–‚»•]', ' ', line)\n",
    "    line = re.sub(\"’\", \"'\", line)\n",
    "    line = re.sub(r\"(\\w+)('[sd(ll)t(ve)(re)m])\", r\"\\g<1> \\g<2>\", line)\n",
    "    currency_regex = r\"[€£]\\d+(,\\d+)*(\\.d+)?[m(bn)]?\"\n",
    "    line = re.sub(currency_regex, CURRENCY_TOKEN, line)\n",
    "    return keras.preprocessing.text.text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~’“—”‘©®™\\t\\n\\xa0', lower=True, split=' ')\n",
    "\n",
    "words = [pretreat(line) for line in corpus[\"Campaign Text\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some transformation must be done on the word instead of the hole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ZyZgnpK4TdX"
   },
   "outputs": [],
   "source": [
    "def transform(word) :\n",
    "    pat = re.compile(\"^'?([^']*)'?$\")\n",
    "    result = re.match(pat,word)\n",
    "    return result.group(1) if result else word\n",
    "\n",
    "clean_words = [[transform(x)  for x in y if transform(x) != \"\"] for y in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words of the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ldZpuO09RRfH",
    "outputId": "405ab794-bb9f-4800-9953-48adf378f3a6"
   },
   "outputs": [],
   "source": [
    "clean_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmYejdUlyz8-"
   },
   "source": [
    "# Load embedding data and create Words dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcolnjAHCL1G"
   },
   "outputs": [],
   "source": [
    "def create_word_dict(lines) :\n",
    "    word_dict = {}\n",
    "    for line in lines : \n",
    "        splt = line.split(\" \")\n",
    "        key = splt[0]\n",
    "        splt[-1] = splt[-1][:-1]\n",
    "        value = np.array(splt[1:],  dtype= np.float32)\n",
    "        word_dict[key] = value\n",
    "    return word_dict\n",
    "\n",
    "with open(words_embedding_path, \"r\", encoding=\"utf8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "word_dict = create_word_dict(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUMNFoitcI8l"
   },
   "source": [
    "# Last Cleaning\n",
    "Filter by occurence and replace unknow words by ```UNKNOWN_TOKEN```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpcU4eGwMW1j"
   },
   "outputs": [],
   "source": [
    "df = pd.Series([x for y in clean_words for x in y])\n",
    "counts = df.value_counts()\n",
    "valid_occurences = set(counts.index[counts > 2])\n",
    "clean_words = [[word for word in line if word in valid_occurences] for line in clean_words]\n",
    "clean_words = [[word if (word in word_dict.keys() or word == CURRENCY_TOKEN) else UNKNOWN_TOKEN  for word in line] for line in clean_words]\n",
    "tokens = set([x for y in clean_words for x in y ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjApVHIF9UhG"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokens, word_dict, dim) : \n",
    "    nrows = len(tokens)+ 2\n",
    "    mat = np.zeros(shape = (nrows, dim), dtype=np.float32)\n",
    "    w2i = {}\n",
    "    i2w = {}\n",
    "    w2i[CURRENCY_TOKEN] = 2\n",
    "    i2w[2] = CURRENCY_TOKEN\n",
    "    w2i[UNKNOWN_TOKEN] = 1\n",
    "    i2w[1] = UNKNOWN_TOKEN\n",
    "    tokens.remove(CURRENCY_TOKEN)\n",
    "    tokens.remove(UNKNOWN_TOKEN)\n",
    "    i = 3\n",
    "    for token in tokens :    \n",
    "        vec = word_dict[token]\n",
    "        mat[i,:] = vec\n",
    "        w2i[token] = i\n",
    "        i2w[i] = token\n",
    "        i+=1\n",
    "\n",
    "    return (w2i, i2w, mat)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBUbcJWBfo_9"
   },
   "source": [
    "# What's on the label side ? \n",
    "Due to the embalanced nature of the dataset's labels, We apply here some weightings to privilege the rare labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "9DTlj8Sifsld",
    "outputId": "717fbe17-c6ee-4d51-be25-9ab1f5cc014b"
   },
   "outputs": [],
   "source": [
    "labels = corpus[\"LEAD ARCHETYPE\"]\n",
    "inverse_weights = 1/labels.value_counts()\n",
    "norm_weights = inverse_weights/ sum(inverse_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-score function\n",
    "We create a function that calculate the F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22T4EgXYy51G"
   },
   "source": [
    "# Dataset creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace word by indexes and pad\n",
    "* Replace words by indexes\n",
    "* Pad with zeros in the end of each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AS3VOJhwxiV"
   },
   "outputs": [],
   "source": [
    "w2i, i2w, mat = create_embedding_matrix(tokens, word_dict, EMBEDDING_DIM)\n",
    "index_words =  [[w2i[x]  for x in y] for y in clean_words]\n",
    "max_text_length = max([ len(x) for x in  index_words])\n",
    "padded_seq = keras.preprocessing.sequence.pad_sequences(index_words,maxlen=None ,padding ='post', truncating = 'post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mE4IM5MRHliS"
   },
   "source": [
    "## Split data\n",
    "Split the data into a train and test dataset, the shuffle the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpXOlQl4Hn5r"
   },
   "outputs": [],
   "source": [
    "def split_data(dataset, labels, train_ratio):\n",
    "    unique_labels = labels.unique()\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels= []\n",
    "    for label in unique_labels : \n",
    "        lab_data = dataset[labels == label]\n",
    "        train, test = train_test_split(lab_data, train_size = train_ratio)\n",
    "        train_data.extend(train)\n",
    "        train_labels.extend(np.repeat([label], len(train)))\n",
    "        test_data.extend(test)\n",
    "        test_labels.extend(np.repeat([label], len(test)))\n",
    "    return (np.array(train_data),np.array(train_labels), np.array(test_data), np.array(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "65pq9_wOIFzq"
   },
   "outputs": [],
   "source": [
    "train_data, ftrain_labels, test_data, ftest_labels = split_data(padded_seq, labels, 0.7)\n",
    "weights =np.asarray([norm_weights[x] for x in ftrain_labels])\n",
    "num_train_data = train_data.shape[0]\n",
    "num_test_data = test_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYEG-gDv11y1"
   },
   "outputs": [],
   "source": [
    "label_encoder = OneHotEncoder()\n",
    "train_labels = label_encoder.fit_transform(np.array(ftrain_labels).reshape(-1,1))\n",
    "test_labels = label_encoder.transform(np.array(ftest_labels).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels.A, weights))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels.A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vU1BDUYdwJ8l"
   },
   "source": [
    "# Simple CNN\n",
    "Now we train a first model using a simple CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZuDaiXoIum4Q"
   },
   "outputs": [],
   "source": [
    "Simple_CNN_BATCH = 21\n",
    "Simple_CNN_NUM_EPOCHS = 15\n",
    "simpleCnnModel = bulid_simple_CNN(mat, max_text_length,  [2,3], 80, \"tanh\", 0.3, True, 4 )\n",
    "simpleCnnModel.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy', f1_m])\n",
    "SCNN_train_data = train_dataset.shuffle(num_train_data).batch(Simple_CNN_BATCH).repeat()\n",
    "SCNN_test_data = test_dataset.batch(num_test_data).repeat()\n",
    "hist = simpleCnnModel.fit(SCNN_train_data,epochs= Simple_CNN_NUM_EPOCHS, \n",
    "                          steps_per_epoch=num_train_data/Simple_CNN_BATCH, \n",
    "                          validation_data= SCNN_test_data, validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlUHase9wYSs"
   },
   "source": [
    "# HCAN\n",
    "Now we train our model with the HCAN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model with 1 hierarchy\n",
    "We first train it on words directely, thus there is only one hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "4zuOOh4IXLm4",
    "outputId": "07b42d13-8866-432f-ddab-dbb6bc0c469f"
   },
   "outputs": [],
   "source": [
    "hcan_model = build_HCAN(max_text_length,mat, 4, 10)\n",
    "hcan_model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam',  \n",
    "                   metrics=['accuracy', f1_m])\n",
    "hcan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGvCxkKrlwTo"
   },
   "outputs": [],
   "source": [
    "HCAN1_BATCH = 7\n",
    "HCAN1_NUM_EPOCHS = 4\n",
    "HCAN1_train_data = train_dataset.shuffle(num_train_data).batch(HCAN1_BATCH).repeat()\n",
    "HCAN1_test_data = test_dataset.batch(num_test_data).repeat()\n",
    "hcan_hist = hcan_model.fit(HCAN1_train_data,epochs= HCAN1_NUM_EPOCHS, steps_per_epoch=num_train_data/HCAN1_BATCH, \n",
    "                           validation_data= HCAN1_test_data, validation_steps=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model with 2 hierarchies\n",
    "We spit each entry into 21 chunks. This could be done more efficiently for example by splitting by sentence in the corpus. This is done to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJ8eqDEZmcC8"
   },
   "outputs": [],
   "source": [
    "splt = 10\n",
    "HCAN2_BATCH = 21\n",
    "HCAN2_NUM_EPOCHS = 30\n",
    "strain_data = np.array([np.array_split(x, splt) for x in train_data])\n",
    "stest_data =  np.array([np.array_split(x, splt) for x in test_data])\n",
    "strain_dataset = tf.data.Dataset.from_tensor_slices((strain_data, train_labels.A, weights))\n",
    "stest_dataset = tf.data.Dataset.from_tensor_slices((stest_data, test_labels.A))\n",
    "HCAN2_train_data = strain_dataset.shuffle(num_train_data).batch(HCAN2_BATCH).repeat()\n",
    "HCAN2_test_data = stest_dataset.batch(num_test_data).repeat()\n",
    "\n",
    "hcan_model2 = build_HCAN(strain_data.shape[1:],mat, 4, 5)\n",
    "\n",
    "hcan_model2.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy', f1_m])\n",
    "hcan_model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "YyJnhff_i5ju",
    "outputId": "572e9caa-f3ca-4730-93a3-38e4ab3aa2ad"
   },
   "outputs": [],
   "source": [
    "hcan2_hist = hcan_model2.fit(HCAN2_train_data,epochs= HCAN2_NUM_EPOCHS, \n",
    "                             steps_per_epoch=num_train_data/HCAN2_BATCH, \n",
    "                           validation_data= HCAN2_test_data, validation_steps=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_HCAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
